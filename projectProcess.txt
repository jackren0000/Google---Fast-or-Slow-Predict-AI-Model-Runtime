
**************************************************
Project Description:
AI model can be represented as a graph, where a node is a tensor operation (i.e. matrix multiplication, convolution, ect),
and an edge represents a tensor.

A compilation configuration controls how the compiler transforms the graph for a specific optimization pass.
There are two types of configuration/optimizations:

1. A layout configuration: control how tensors in the graph are laid out in the physical memory, by specifying the dimension order 
of each input and output of an operation node.

Add Operation: Expected to add two tensors of shape [2,4,16] together, resulting in a tensor of the same shape [2,4,16].

Max Operation: Adjusts the tensor shape to [4,16,8]. The configuration {1,0,2} suggests a reordering of the tensor dimensions from [2,4,16] to [4,2,16].

Reshape Operation: Alters the tensor shape to [128]. The configurations {0}, (0,1,2}, and (0,2,1) imply that the output tensor keeps its original layout, and for the input tensors, the first keeps its original layout while the second one has its second and third dimensions swapped.

Convolution (Conv) Operation: Applies a convolution to the input tensor, producing an output tensor of shape [2,8]. The {1,0} configuration suggests that the first and second dimensions of the output tensor are swapped.

2





**************************************************
About the dataset:
TpuGraphs is the performance prediction dataset on XLA HLO graphs running on TPU (Tensor Processing Units) V3.

There are 5 data collections in total:
1. layout:xla:random
2. layout:xla:default
3. layout:nlp:random
4. layout:nlp:default
5. title: xla

*********Notice******* 
The final score is the average across all collections.
dataset: https://www.kaggle.com/competitions/predict-ai-model-runtime/data

Q&A:
1. what is XLA?
XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with 
potentially no source code changes. For instance, "tf.reduce_sum(x + y * z)", XLA fuse the multiplication, addition and reduction
into a signle kernel instead of three.
